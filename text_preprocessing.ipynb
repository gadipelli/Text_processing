{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download all required libs\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '''Even students who are just learning to write, such as kindergartners, can benefit from \n",
    "        understanding the structure of a paragraph. As they progress into the later years of elementary\n",
    "        school and middle school, they can begin to create paragraphs that include all of the necessary \n",
    "        Once a student reaches high school, they should be able to write a paragraph that has a d\n",
    "        istinct thesis and also includes supporting arguments.Hello! I’m Journal Buddies Jill, and I am so glad \n",
    "        that you found my blog. You have discovered a resource of 12,000+ free writing ideas and journal \n",
    "        prompts! Most of the prompts are for kids and students, but some are for writers of all ages. Take a\n",
    "        look around and enjoy!'One way that you can help students practice their paragraph writing skills is \n",
    "        to introduce paragraph writing topics. These paragraph writing topics can be used for nearly any type \n",
    "        of classroom, regardless of grade level or subject matter:Write a paragraph about an important person\n",
    "        in history. Be sure to address the topic of the paragraph, such as that person’s career, birthplace \n",
    "        or prominent contribution. Use supporting sentences to expand on the topic chosen.\n",
    "        Write a paragraph about your bedroom at home. Be descriptive, and use adjectives to describe how the \n",
    "        space looks and how you feel when you are in your bedroom.\n",
    "        Write a paragraph about a holiday that you do not celebrate. Use facts within the supporting sentences\n",
    "        to explain the holiday and the traditions that are associated with it.\n",
    "        Write a paragraph about an insect. Make sure to use scientific data and observations to create strong \n",
    "        support within the paragraph.\n",
    "        Write a paragraph arguing your opinion on a controversial topic. Make sure to use factual information to \n",
    "        support your opinion, and conclude with why you feel the way that you do.\n",
    "        Write a descriptive paragraph about your garden. Identify the plants that are in the garden, and use \n",
    "        descriptive phrases to make the reader feel as if they are walking through your garden.\n",
    "        Write a paragraph providing instructions on how to code your favorite game. Be sure to offer clear direction,\n",
    "        and don’t forget to use transitional phrases to guide the reader from one step to the next.\n",
    "        Write a paragraph about a new invention that you would create. Use descriptive phrases to describe your \n",
    "        invention and to support the topic.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentesnces, words makinf from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#step1: creating sentences from corpus\n",
    "sent= nltk.sent_tokenize(corpus)\n",
    "for sente in sent:\n",
    "    print(sente)\n",
    "    \n",
    "#step2: create token_words from corpus\n",
    "token = nltk.word_tokenize(corpus)\n",
    "for tokens in token:\n",
    "    print(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lematization and stemning and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemning,lematization,stopwords removal.\n",
    "'''ex   stemmning     :finally, final, finalized == fina\n",
    "        lemmatization :finally, final, finalized == final \n",
    "        stopwords     :off,and, an, a, the, from, etc. these words are not help full to prediction'''\n",
    "        #here lemmatization is good to use always becuse stemm will give meaning less word formations.\n",
    "\n",
    "    \n",
    "\n",
    "'''###stop words###'''\n",
    "from nltk.corpus import stopwords\n",
    "sents = nltk.sent_tokenize(corpus)\n",
    "stopwords.words('english')#it will prints the all stop words in english   \n",
    "\n",
    "'''###stemning###'''\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for i in range(len(sents)):\n",
    "    words = nltk.word_tokenize(sents[i])\n",
    "    words= [stemmer.stem(word)for word in words if word not in set(stopwords.words('english'))]\n",
    "    sents[i] = ' '.join(words)\n",
    "print(sents)\n",
    "\n",
    "'''###lematization###'''\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(sent)):\n",
    "    words = nltk.word_tokenize(sents[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sents[i] = ' '.join(words)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''here we will find th word frequency for each sentence according each word.\n",
    "   in bag of words every word become a feature. but we loss word imaprtance.\n",
    "   here every word will get same representation (0 or 1)it wont work well in large datasets'''\n",
    "sentss = nltk.sent_tokenize(corpus)\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "corpus1 = []\n",
    "for i in range(len(sentss)):\n",
    "    reviews = re.sub('[^a-zA-Z]', ' ',sents[i]) # removing all puncthuations(,.:'>')\n",
    "    reviews = reviews.lower()#converting lower case\n",
    "    reviews = reviews.split()#splitting each sentence\n",
    "    reviews = [lem.lemmatize(word) for word in reviews if word not in set(stopwords.words('english'))]\n",
    "    reviews = ' '.join(reviews)\n",
    "    corpus1.append(reviews)\n",
    "print(corpus1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''apply bag of words concept here(count vectorizer)'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bag_of_words = CountVectorizer()\n",
    "x = bag_of_words.fit(corpus1).toarry()\n",
    "print(bag_of_words.get_feature_names())\n",
    "# this features are used in ml model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf(time frequency and inverse document frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"formula = tf*log(idf)'''\n",
    "'''why log in idf:\n",
    "becuse which transform actual big values to low values. so that the domination of idf is reduces.\n",
    "for every word we nedd to caluculate the tf-idf'''\n",
    "#term frequency\n",
    "'''no of repetations of words in a sentence / no of words in sentence'''\n",
    "#inverse document frequnce\n",
    "'''log(no of sentences/no of sentences containg word)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''apply bag of words concept here(count vectorizer)'''\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "bag_of_words = TfidfVectorizer()\n",
    "x = bag_of_words.fit(corpus1).toarry()\n",
    "print(bag_of_words.get_feature_names())\n",
    "# this features are used in ml model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word to vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntokenization\\ncreate histograms\\ntake most frequent words\\ncreate a matrix with all unique words it also represnts the occurance relationship between words'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''in this method each word represented as a vector od 32 dimentions instead of a single number\n",
    "   also here the semantic words and the relation between words is preserved.'''\n",
    "'''word2vec is a neural network model. it will uses CBOW(continuous bag of words) and SKIPGRAM'''\n",
    "#steps to do word-vec\n",
    "'''\n",
    "tokenization\n",
    "create histograms\n",
    "take most frequent words\n",
    "create a matrix with all unique words it also represnts the occurance relationship between words'''\n",
    "#limitations for W2V is:\n",
    "#it will get vectors for presented corpus words only.\n",
    "\n",
    "from gensim.models import Word2Vec #importing lib\n",
    "W2V_model = Word2Vec(sentences, min_count=1)#two parameters\n",
    "print(W2V_model)\n",
    "\n",
    "words = list(W2V_model.WV.vocab)# creating vocabluary \n",
    "print(words)\n",
    "\n",
    "print(W2V_model['give_any_vord'])\n",
    "W2V_model.most_simmilar('war')#finding simmilar vectors related to words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating word embeding model using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array \n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "type1 word2vec(using gensim)\n",
    "note:type2 word2vec method is in part of building lstm model(there we will import using word embedings )\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import show, figure\n",
    "%matplotlib inline\n",
    "my_model = Word2Vec(tokens,min_count=1,size=64)\n",
    "words = list(my_model.wv.vocab)\n",
    "len(words)\n",
    "my_model.wv.most_similar('class')\n",
    "my_model.wv.most_similar('trump')\n",
    "my_model.wv.most_similar('usa')\n",
    "my_model.wv.most_similar('day')\n",
    "my_model.wv.most_similar('month')\n",
    "#to find any query is not presented in text\n",
    "my_model.wv.doesnt_match('abraham mahesh is under the bags'.split())\n",
    "#to find the values between two words\n",
    "my_model.wv.similarity('class','book')\n",
    "my_model.wv.similarity('day','night')\n",
    "my_model.wv.most_similar(positive = ['son','mom'],negative=['father'])\n",
    "#these are 64 dimentions of vector representation\n",
    "my_model.wv['king']\n",
    "reduce dimetions with tsne\n",
    "X = my_model.wv[my_model.wv.vocab]\n",
    "X.shape\n",
    "tsne = TSNE(n_components=2,n_iter=250)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "#create a dataframe to show transformed dimentions\n",
    "wv_dims = pd.DataFrame(x_2d,columns=['x1','x2'])\n",
    "wv_dims['words']=my_model.wv.vocab.keys()\n",
    "wv_dims.head()\n",
    "wv_dims.plot.scatter('x','y',figsize=(8,8), marker = '.',s=10,alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
